{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from linear_algebra import distance\n",
    "from stats import mean\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's say we've picked a number k like 3 or 5. Then when we want to classify some new data point,we find the k nearest labeled\n",
    "# points and let them vote on the new output.To do this, we'll need a function that counts votes. One possibility is:\n",
    "def raw_majority_vote(labels):\n",
    "    votes=Counter(labels)\n",
    "    winner,_=votes.most_common(1)[0]\n",
    "    return winner\n",
    "\n",
    "# But this doesn't do anything intelligent with ties. For example, imagine we're rating movies and the five nearest movies\n",
    "# are rated G, G, PG, PG, and R. Then G has two votes and PG also has two votes. In that case, we have several options:\n",
    "# 1. Pick one of the winners at random.\n",
    "# 2. Weight the votes by distance and pick the weighted winner.\n",
    "# 3. Reduce k until we find a unique winner.\n",
    "\n",
    "# We'll implement the third:\n",
    "def majority_vote(labels):\n",
    "    \"\"\"assumes that labels are ordered from nearest to farthest\"\"\"\n",
    "    votes=Counter(labels)\n",
    "    winner, winner_count = votes.most_common(1)[0]\n",
    "    num_winners=len([count for count in votes.values() if count == winner_count])\n",
    "    if num_winners == 1:\n",
    "        return winner                         # unique winner, so return it\n",
    "    else:\n",
    "        return majority_vote(labels[:-1])     # try again without the farthest\n",
    "# This approach is sure to work eventually, since in the worst case we go all the way down to just one label,\n",
    "# at which point that one label wins.\n",
    "\n",
    "# With this function it's easy to create a classifier:\n",
    "def knn_classify(k,labeled_points,new_point):\n",
    "    \"\"\"each labeled point should be a pair (point, label)\"\"\"\n",
    "    # order the labeled points from nearest to farthest\n",
    "    by_distance=sorted(labeled_points,key=lambda point:distance(new_point,point[0]))\n",
    "    # find the labels for the k closest\n",
    "    k_nearest_labels=[label for _,label in by_distance[:k]]\n",
    "#   print k_nearest_labels\n",
    "    # and let them vote\n",
    "    return majority_vote(k_nearest_labels)\n",
    "\n",
    "\n",
    "# Let’s take a look at how this works.\n",
    "# Example: Favorite Languages\n",
    "# The results of the first DataSciencester user survey are back, and we’ve found the preferred programming languages \n",
    "# of our users in a number of large cities:\n",
    "'''\n",
    "cities=[([-122.3,47.3],\"Python\"),([-96.85,32.85],\"Java\"),([-89.33,43.13],\"R\"),([-136.33,43.13],\"Python\"),\n",
    "          ([-91.85,34.85],\"Java\"),([-141.3,53.3],\"Python\")]\n",
    "'''\n",
    "\n",
    "'''\n",
    "new_point=[-143,40]\n",
    "print knn_classify(2,cities,new_point)\n",
    "'''\n",
    "\n",
    "# The VP of Community Engagement wants to know if we can use these results to predict the favorite programming languages \n",
    "# for places that weren’t part of our survey.\n",
    "\n",
    "# As usual, a good first step is plotting the data (Figure 12-1):\n",
    "# key is language, value is pair (longitudes, latitudes)\n",
    "'''\n",
    "plots = { \"Java\" : ([], []), \"Python\" : ([], []), \"R\" : ([], []) }\n",
    "\n",
    "# we want each language to have a different marker and color\n",
    "markers = { \"Java\" : \"o\", \"Python\" : \"s\", \"R\" : \"^\" }\n",
    "colors = { \"Java\" : \"r\", \"Python\" : \"b\", \"R\" : \"g\" }\n",
    "\n",
    "for (longitude, latitude), language in cities:\n",
    "    plots[language][0].append(longitude)\n",
    "    plots[language][1].append(latitude)\n",
    "print plots\n",
    "\n",
    "# create a scatter series for each language\n",
    "for language, (x, y) in plots.iteritems():\n",
    "    plt.scatter(x, y, color=colors[language], marker=markers[language],\n",
    "                        label=language, zorder=10)\n",
    "# plot_state_borders(plt) # pretend we have a function that does this\n",
    "\n",
    "plt.legend(loc=0)                   # let matplotlib choose the location\n",
    "plt.axis([-130,-60,20,55])          # set the axes\n",
    "plt.title(\"Favorite Programming Languages\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "'''\n",
    "# try several different values for k. ???? Try when dataset is there.\n",
    "for k in [1, 3, 5, 7]:\n",
    "num_correct = 0\n",
    "    for city in cities:\n",
    "        location, actual_language = city\n",
    "        other_cities = [other_city for other_city in cities if other_city != city]\n",
    "        predicted_language = knn_classify(k, other_cities, location)\n",
    "        if predicted_language == actual_language:\n",
    "            num_correct += 1\n",
    "    print k, “neighbor[s]:”, num_correct, “correct out of”, len(cities)\n",
    "# It looks like 3-nearest neighbors performs the best, giving the correct result about 59% of the time:\n",
    "# 1 neighbor[s]: 40 correct out of 75\n",
    "# 3 neighbor[s]: 44 correct out of 75\n",
    "# 5 neighbor[s]: 41 correct out of 75 \n",
    "# 7 neighbor[s]: 35 correct out of 75\n",
    "'''\n",
    "\n",
    "# Now we can look at what regions would get classified to which languages under each nearest neighbors scheme.\n",
    "# We can do that by classifying an entire grid worth of points, and then plotting them as we did the cities:\n",
    "'''\n",
    "plots = { \"Java\" : ([], []), \"Python\" : ([], []), \"R\" : ([], []) }\n",
    "k = 1   # or 3, or 5, or…\n",
    "for longitude in range(-130, -60):\n",
    "    for latitude in range(20, 55):\n",
    "        predicted_language = knn_classify(k, cities, [longitude, latitude])\n",
    "        plots[predicted_language][0].append(longitude)\n",
    "        plots[predicted_language][1].append(latitude)\n",
    "'''        \n",
    "# For instance, Figure 12-2 shows what happens when we look at just the nearest neighbour(k=1).\n",
    "# Figure 12-3. 3-Nearest neighbor programming languages\n",
    "# Figure 12-4. 5-Nearest neighbor programming languages\n",
    "    \n",
    "# The Curse of Dimensionality:\n",
    "# k-nearest neighbors runs into trouble in higher dimensions thanks to the \"curse of dimensionality\", which boils down to the fact\n",
    "# that high-dimensional spaces are vast. Points in high-dimensional spaces tend not to be close to one another at all. \n",
    "# One way to see this is by randomly generating pairs of points in the d-dimensional “unit cube” in a variety of dimensions, \n",
    "# and calculating the distances between them.\n",
    "def random_point(dim): \n",
    "    return [random.random() for _ in range(dim)]\n",
    "\n",
    "# as is writing a function to generate the distances:\n",
    "def random_distances(dim, num_pairs):\n",
    "    return [distance(random_point(dim), random_point(dim)) for _ in range(num_pairs)]\n",
    "\n",
    "# For every dimension from 1 to 100, we’ll compute 10,000 distances and use those to compute the average distance between \n",
    "# points and the minimum distance between points in each dimension (Figure 12-5):\n",
    "\n",
    "''' \n",
    "dimensions = range(1, 101)\n",
    "avg_distances = []\n",
    "min_distances = []\n",
    "random.seed(0)\n",
    "for dim in dimensions:\n",
    "    distances = random_distances(dim, 10000) # 10,000 random pairs\n",
    "    avg_distances.append(mean(distances))    # track the average\n",
    "    min_distances.append(min(distances))     # track the minimum\n",
    "''' \n",
    "\n",
    "# Skipped ???? Do later as very important\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
