{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: cp1252 -*- ????\n",
    "from __future__ import division\n",
    "import math\n",
    "from functools import partial\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************\n",
    "# **********************Chapter 1. Introduction****************************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "users = [\n",
    "{ \"id\": 0, \"name\": \"Hero\" },\n",
    "{ \"id\": 1, \"name\": \"Dunn\" },\n",
    "{ \"id\": 2, \"name\": \"Sue\" },\n",
    "{ \"id\": 3, \"name\": \"Chi\" },\n",
    "{ \"id\": 4, \"name\": \"Thor\" },\n",
    "{ \"id\": 5, \"name\": \"Clive\" },\n",
    "{ \"id\": 6, \"name\": \"Hicks\" },\n",
    "{ \"id\": 7, \"name\": \"Devin\" },\n",
    "{ \"id\": 8, \"name\": \"Kate\" },\n",
    "{ \"id\": 9, \"name\": \"Klein\" }\n",
    "]\n",
    "\n",
    "friendships = [(0, 1), (0, 2), (1, 2), (1, 3), (2, 3), (3, 4),(4, 5), (5, 6), (5, 7), (6, 8), (7, 8), (8, 9)]\n",
    "\n",
    "#1. For example, we might want to add a list of friends to each user. First we set each user’s friends property to an empty list:\n",
    "for user in users:\n",
    "    user[\"friends\"] = []\n",
    "\n",
    "for i, j in friendships:\n",
    "    # this works because users[i] is the user whose id is i\n",
    "    users[i][\"friends\"].append(users[j]) \t# add i as a friend of j\n",
    "    users[j][\"friends\"].append(users[i]) \t# add j as a friend of i\n",
    "\n",
    "#2.\n",
    "# Once each user dict contains a list of friends, we can easily ask questions of our graph, like “what’s the average number of connections?”\n",
    "# First we find the total number of connections, by summing up the lengths of all the friends lists:\n",
    "def number_of_friends(user):\n",
    "    \"\"\"how many friends does _user_ have?\"\"\"\n",
    "#    print len(user[\"friends\"])\n",
    "    return len(user[\"friends\"])\t\t\t\t\t         # length of friend_ids list\n",
    "total_connections = sum(number_of_friends(user) for user in users) \t # 24\n",
    "# print 'total_connections are %r ' %total_connections\n",
    "\n",
    "# And then we just divide by the number of users:\n",
    "num_users = len(users) \t\t\t\t\t# length of the users list\n",
    "avg_connections = total_connections / num_users\t# 2.4\n",
    "# print 'avg_connections are %r ' %avg_connections\n",
    "\n",
    "\n",
    "#3.\n",
    "# It’s also easy to find the most connected people - they’re the people who have the largest number of friends.\n",
    "# Since there aren’t very many users, we can sort them from “most friends” to “least friends”:\n",
    "# create a list (user_id, number_of_friends)\n",
    "num_friends_by_id=[(user['id'],number_of_friends(user)) for user in users]\n",
    "# print sorted(num_friends_by_id,          # get it sorted\n",
    "#             key=lambda x:x[1],          # by num_friends\n",
    "#             reverse=True)               # largest to smallest         \n",
    "\n",
    "\n",
    "#4.\n",
    "#VP asks you to design a “Data Scientists You May Know” suggester. Your first instinct is to suggest that a user might know the friends of friends.\n",
    "#These are easy to compute: for each of a user’s friends, iterate over that person’s friends, and collect all the results:\n",
    "def friends_of_friend_ids_bad(user):\n",
    "    # \"foaf\" is short for \"friend of a friend\"\n",
    "    return [foaf['id']\n",
    "            for friend in user['friends']    # for each of user's friends\n",
    "            for foaf in friend['friends']]   # get each of _their_ friends\n",
    "# print friends_of_friend_ids_bad(users[0]) \n",
    "\n",
    "\n",
    "# ???? skipped the remaining chapter right now. However should be completed later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************\n",
    "# **********************Chapter 2. A Crash Course in Python*****************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "# Skipped ????. As already learning python but can be done later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#***************************************************************************************************\n",
    "# **********************Chapter 3. Visualizing Data*************************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "# Skipped ????. Can be done after installing Matplotlib in python 2.7 or can be done in Anaconda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nidentity_matrix = make_matrix(5,5,is_diagonal)\\nprint identity_matrix\\nprint make_matrix(5,4,is_diagonal)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#***************************************************************************************************\n",
    "# **********************Chapter 4. Linear Algebra***************************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "#1. Add Vectors\n",
    "def vector_add(v,w):\n",
    "    return [v_i+w_i for v_i,w_i in zip(v,w)]\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "vector_a_sum_b=vector_add(a,b)\n",
    "print vector_a_sum_b\n",
    "'''\n",
    "\n",
    "#2. Subtract Vectors\n",
    "def vector_subtract(v,w):\n",
    "    return [v_i-w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "#3. Sum of all vectors\n",
    "def vector_sum(vectors):\n",
    "    result=vectors[0]\n",
    "    for vector in vectors[1:]:\n",
    "        result=vector_add(result,vector)\n",
    "    return result\n",
    "#Above can be written as below as well:\n",
    "#def vector_sum(vectors):\n",
    "#    return reduce(vector_add, vectors)\n",
    "\n",
    "#or even\n",
    "#vector_sum = partial(reduce, vector_add)\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "c=[7,8,9]\n",
    "vectors=[a,b,c]\n",
    "vector_sum_all=vector_sum(vectors)\n",
    "print vector_sum_all\n",
    "'''\n",
    "\n",
    "#4. Multiply vactor by a scaler\n",
    "def scalar_multiply(c,v):\n",
    "    \"\"\"C is a number and v is a vector\"\"\"\n",
    "    return [c*v_i for v_i in v]\n",
    "#print scalar_multiply.__doc__\n",
    "\n",
    "#5. Calculate mean of vectors\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"compute the vector whose ith element is the mean of the ith elements of the input vectors\"\"\"\n",
    "    n=len(vectors)\n",
    "    return scalar_multiply(1/n,vector_sum(vectors))\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "c=[7,8,9]\n",
    "vectors=[a,b,c]\n",
    "mean_of_vectors= vector_mean(vectors)\n",
    "print mean_of_vectors\n",
    "'''\n",
    "\n",
    "#6. Dot product of vectors. Another way of saying this is that it’s the length of the vector you’d get if you projected v onto w.\n",
    "def dot(v,w):\n",
    "    \"\"\"v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    return sum([v_i*w_i for v_i,w_i in zip(v,w)])\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "vector_a_dot_b=dot(a,b)\n",
    "print vector_a_dot_b\n",
    "'''\n",
    "\n",
    "#7. sum of squares of vector\n",
    "def sum_of_squares(v):\n",
    "    \"\"\"v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v,v)\n",
    "'''\n",
    "a=[1,2,3]\n",
    "sum_of_square_vectors=sum_of_squares(a)\n",
    "print sum_of_square_vectors\n",
    "'''\n",
    "\n",
    "#8. it’s easy to compute a vector’s sum of squares. Which we can use to compute its magnitude (or length)\n",
    "#import math\n",
    "def magnitude(v):\n",
    "    return math.sqrt(sum_of_squares(v))  # math.sqrt is square root function\n",
    "'''\n",
    "a=[1,2,3]\n",
    "magnitude_vectors=magnitude(a)\n",
    "print magnitude_vectors                          \n",
    "'''\n",
    "\n",
    "#9. We now have all the pieces we need to compute the distance between two vectors\n",
    "def squared_distance(v, w):\n",
    "    \"\"\"(v_1 - w_1) ** 2 + ... + (v_n - w_n) ** 2\"\"\"\n",
    "    return sum_of_squares(vector_subtract(v,w))\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "vector_squared_distance=squared_distance(a,b)\n",
    "print vector_squared_distance\n",
    "'''\n",
    "\n",
    "#10. distance between 2 vectors\n",
    "#def distance(v, w):\n",
    "#    return math.sqrt(squared_distance(v, w))\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "vector_distance=distance(a,b)\n",
    "print vector_distance\n",
    "'''\n",
    "\n",
    "#11. Which is possibly clearer if we write it as (the equivalent):\n",
    "def distance(v, w):\n",
    "    return magnitude(vector_subtract(v, w))\n",
    "'''\n",
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "vector_distance=distance(a,b)\n",
    "print vector_distance\n",
    "'''\n",
    "\n",
    "\n",
    "# Matrices------------------------------------------>\n",
    "\n",
    "''' Because we’re representing matrices with Python lists, which are zero-indexed, we’ll call the first row of a matrix “row 0”\n",
    "and the first column “column 0.” '''\n",
    "\n",
    "#1. Get Shape of matrix. Given this list-of-lists representation, the matrix A has len(A) rows and len(A[0]) columns, which we consider its shape.\n",
    "def shape(A):\n",
    "    no_rows=len(A)\n",
    "    no_columns=len(A[0]) if A else 0\n",
    "    return no_rows,no_columns\n",
    "'''\n",
    "A=[[1,2,3],[4,5,6]]\n",
    "print shape(A)\n",
    "'''\n",
    "\n",
    "''' If a matrix has n rows and k columns, we will refer to it as a n*k matrix. We can (and sometimes will)\n",
    "think of each row of a n*k matrix as a vector of length k, and each column as a vector of length n '''\n",
    "#2. get the ith row or jth column\n",
    "def get_row(A,i):\n",
    "    return A[i]                     # A[i] is already the ith row\n",
    "\n",
    "def get_column(A,j):\n",
    "    return [A_i[j]                  # jth element of row A_i\n",
    "            for A_i in A]           # for each row A_i\n",
    "\n",
    "''' We’ll also want to be able to create a matrix given its shape and a function for generating its elements. We can do this using a nested list\n",
    "comprehension. we’ll see later, we can use an n*k matrix to represent a linear function that maps k-dimensional vectors to n-dimensional vectors.\n",
    "Several of our techniques and concepts will involve such functions.\n",
    "'''\n",
    "#3. Make a matrix\n",
    "def make_matrix(total_rows,total_columns,entry_fn):\n",
    "    \"\"\"returns a num_rows x num_cols matrix whose (i,j)th entry is entry_fn(i, j)\"\"\"\n",
    "    return [[entry_fn(i,j) for j in range(total_columns)] for i in range(total_rows)]\n",
    "\n",
    "#4. Function to return element to generate matrix\n",
    "def is_diagonal(i,j):\n",
    "    \"\"\"1's on the 'diagonal', 0's everywhere else\"\"\"\n",
    "    return 1 if i==j else 0\n",
    "'''\n",
    "identity_matrix = make_matrix(5,5,is_diagonal)\n",
    "print identity_matrix\n",
    "print make_matrix(5,4,is_diagonal)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nn=15\\na=range(1,n)\\nb=range(1,n)             \\nc=[-n+i for i in a]\\nd=[1,3,4,5,6,2,7,8,9,10,11,13,12,14]\\nprint 'a is', a\\nprint 'b is', b\\nprint 'c is', c\\nprint 'd is', d\\nprint 'covariance is: ',covariance(a,b)\\nprint 'correlation is: ',correlation (a,b)                \\nprint 'covariance is: ',covariance(a,c)                \\nprint 'correlation is: ',correlation (a,c)\\nprint 'covariance is: ',covariance(a,c)                \\nprint 'correlation is: ',correlation (a,c)\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#***************************************************************************************************\n",
    "#*********************************Chapter 5. Statistics*********************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "'''\n",
    "num_friends    = [100, 49, 41, 40, 25, ... and lots more]\n",
    "num_points     = len(num_friends) # 204\n",
    "largest_value  = max(num_friends) # 100\n",
    "smallest_value = min(num_friends) # 1\n",
    "sorted_values  = sorted(num_friends)\n",
    "smallest_value = sorted_values[0] # 1\n",
    "second_smallest_value = sorted_values[1] # 1\n",
    "second_largest_value  = sorted_values[-2] # 49\n",
    "'''\n",
    "\n",
    "# Central Tendencies------------------->\n",
    "# 1. Mean: Usually, we’ll want some notion of where our data is centered. Most commonly we’ll use the mean (or average), # which is just the sum of the data\n",
    "# divided by its count:\n",
    "\n",
    "def mean(x):\n",
    "    return sum(x)/len(x)\n",
    "\n",
    "#2. Median\n",
    "def median(x):\n",
    "    total_elements=len(x)\n",
    "    \"\"\"NOTE: There are, in fact, nobvious tricks to efficiently compute medians without sorting the data.\n",
    "    However, they are beyond the scope of this book, so we have to sort the data.\n",
    "    A generalization of the median is the quantile, which represents the value less than which a certain percentile of the data lies.\n",
    "    The median represents the value less than which 50% of the data lies \"\"\"\n",
    "    sorted_x=sorted(x)\n",
    "    if total_elements%2==0:\n",
    "        return (sorted_x[(total_elements//2-1)]+sorted_x[total_elements//2])/2\n",
    "    else:\n",
    "        return sorted_x[total_elements//2]\n",
    "'''\n",
    "num_friends_1 = [2, 3, 4, 2, 4]\n",
    "num_friends_2 = [2, 3, 4, 2, 4,5]\n",
    "print mean(num_friends_1)\n",
    "print median(num_friends_1)\n",
    "print median(num_friends_2)\n",
    "'''\n",
    "\n",
    "#3. Quantile. I think can be called percentile as well.\n",
    "def quantile(x,p):\n",
    "    \"\"\"A generalization of the median is the quantile, which represents the value less than which a certain percentile of the data lies. \"\"\"\n",
    "    index=int(p*len(x))\n",
    "    return sorted(x)[index]\n",
    "'''\n",
    "num_friends_3 = range(101,201)   \n",
    "print quantile(num_friends_3,.20)  \n",
    "'''\n",
    "\n",
    "#4. Less commonly you might want to look at the mode, or most-common value[s]\n",
    "from collections import Counter\n",
    "import random\n",
    "#random.seed(44)\n",
    "def mode(x):\n",
    "    counts=Counter(x)\n",
    "    max_value=max(counts.values())\n",
    "    # print 'max_value', max_value\n",
    "    return [i for i,j in counts.iteritems() if j==max_value]\n",
    "'''\n",
    "a=[random.choice(range(1,100)) for _ in range(100)]\n",
    "print sorted(a)\n",
    "print mode(a)\n",
    "'''\n",
    "\n",
    "# Dispersion------------------->\n",
    "''' Dispersion refers to measures of how spread out our data is. Typically they’re statistics for which values near zero signify not spread out at all\n",
    "and for which large values (whatever that means) signify very spread out. '''\n",
    "#5. Range: Difference between the largest and smallest elements.\n",
    "def data_range(x):\n",
    "    return max(x)-min(x)\n",
    "\n",
    "#6. Variance: A more complex measure of dispersion is the variance.\n",
    "def de_mean(x):\n",
    "    \"\"\"translate x by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar=mean(x)\n",
    "    n=[i-x_bar for i in x]\n",
    "    # print 'in de_mean x-x_bar is', n\n",
    "    return n\n",
    "\n",
    "def variance(x):\n",
    "    \"\"\"assumes x has at least two elements\"\"\"\n",
    "    n=len(x)\n",
    "    mean_deviations=de_mean(x)\n",
    "    return sum_of_squares(mean_deviations)/n-1\n",
    "''' NOTE This looks like it is almost the average squared deviation from the mean, except that we’re dividing by n-1 instead of n.\n",
    "In fact, when we’re dealing with a sample from a larger population, x_bar is only an estimate of the actual mean, which means that\n",
    "on average (x_i-x_bar) ** 2 is an underestimate of x_i’s squared deviation from the mean, which is why we divide by n-1 instead of n. ''' \n",
    "'''\n",
    "a=[random.choice(range(1,100)) for _ in range(100)]\n",
    "print variance(a)\n",
    "'''\n",
    "\n",
    "#7. Standard Deviation: A more complex measure of dispersion is the variance.\n",
    "'''\n",
    "Now, whatever units our data is in (e.g., “friends”), all of our measures of central tendency are in that same unit. The range will similarly\n",
    "be in that same unit. The variance, on the other hand, has units that are the square of the original units (e.g., “friends squared”).\n",
    "As it can be hard to make sense of these, we often look instead at the standard deviation.\n",
    "'''\n",
    "\n",
    "def standard_deviation(x):\n",
    "    return math.sqrt(variance(x))\n",
    "a=[random.choice(range(1,100)) for _ in range(100)]\n",
    "'''\n",
    "print 'Mean value is {}'.format(mean(a))\n",
    "print '50 percentile is {}'.format(quantile(a,.50))\n",
    "print 'standard_deviation is {}'.format(standard_deviation(a))\n",
    "'''\n",
    "\n",
    "#8. Interquartile Range\n",
    "''' Both the range and the standard deviation have the same outlier problem that we saw for the mean earlier for the mean.\n",
    "Using the same example, if our friendliest user had instead 200 friends, the standard deviation would be 14.89, more than 60% higher!\n",
    "A more robust alternative computes the difference between the 75th percentile value and the 25th percentile value. '''\n",
    "def interquartile_range(x):\n",
    "    return quantile(x,.75)-quantile(x,.25)\n",
    "'''\n",
    "a=[random.choice(range(1,100)) for _ in range(100)]\n",
    "print 'Interquartile Range is {}'.format(interquartile_range(a))\n",
    "'''\n",
    "\n",
    "\n",
    "# Correlation------------------->\n",
    "''' Whereas variance measures how a single variable deviates from its mean, covariance measures how two variables vary in tandem from their means'''\n",
    "#8. Covariance: measures how two variables vary in tandem from their means\n",
    "''' Recall that dot sums up the products of corresponding pairs of elements. When corresponding elements of x and y are either both above their means\n",
    "or both below their means, a positive number enters the sum. When one is above its mean and the other below, a negative number enters the sum.\n",
    "Accordingly, a “large” positive covariance means that x tends to be large when y is large and small when y is small. A “large” negative covariance means\n",
    "the opposite — that x tends to be small when y is large and vice versa. A covariance close to zero means that no such relationship exists. '''\n",
    "def covariance(x,y):\n",
    "    n=len(x)\n",
    "    return dot(de_mean(x),de_mean(y))/n-1\n",
    "               \n",
    "#9. Correlation\n",
    "def correlation(x,y):\n",
    "    std_x=standard_deviation(x)\n",
    "    std_y=standard_deviation(y)\n",
    "    if std_x > 0 and std_y > 0:\n",
    "        return covariance(x,y)/std_x/std_y\n",
    "    else:\n",
    "        return 0\n",
    "'''\n",
    "n=15\n",
    "a=range(1,n)\n",
    "b=range(1,n)             \n",
    "c=[-n+i for i in a]\n",
    "d=[1,3,4,5,6,2,7,8,9,10,11,13,12,14]\n",
    "print 'a is', a\n",
    "print 'b is', b\n",
    "print 'c is', c\n",
    "print 'd is', d\n",
    "print 'covariance is: ',covariance(a,b)\n",
    "print 'correlation is: ',correlation (a,b)                \n",
    "print 'covariance is: ',covariance(a,c)                \n",
    "print 'correlation is: ',correlation (a,c)\n",
    "print 'covariance is: ',covariance(a,c)                \n",
    "print 'correlation is: ',correlation (a,c)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************\n",
    "# **********************Chapter 6. Probability***************************************************\n",
    "#***************************************************************************************************\n",
    "\"\"\"\n",
    "We could also ask about the probability of the event “both children are girls” conditional on the event “at least one of the children is a girl” (L). Surprisingly, the answer is different from before!\n",
    "As before, the event B and L (“both children are girls and at least one of the children is a girl”) is just the event B. This means we have:\n",
    "P(B|L)=P(B,L)/P(L)= P(B)/P(L)=1/3\n",
    "My solution: \n",
    "P(B/L)=P(L/B)*P(B)/P(L)\n",
    "P(L/B)=1 probability at least 1 is girl given both are girls 1.\n",
    "P(B)=(1/2)*(1/2)=1/4 Probability that both are girls.\n",
    "P(L)=GirlGirl+BoyGirl+GirlBoy=1/4+1/4+1/4=3/4\n",
    "So P(B)/P(L)=(1/4)*(4/3)=1/3\n",
    "\"\"\"\n",
    "#Prove through program\n",
    "def randon_kid():\n",
    "    return random.choice(['boy','girl'])\n",
    "'''\n",
    "older_girl=0\n",
    "both_girls=0\n",
    "either_girl=0\n",
    "random.seed(0)\n",
    "for _ in xrange(10000):\n",
    "    older=randon_kid()\n",
    "    younger=randon_kid()\n",
    "    if older=='girl':\n",
    "        older_girl+=1\n",
    "    if older=='girl' or younger=='girl':\n",
    "        either_girl+=1\n",
    "    if older=='girl' and younger=='girl':\n",
    "        both_girls+=1\n",
    "print 'P(both girls/older girl ', both_girls/older_girl\n",
    "print 'P(both girls/either girl ', both_girls/either_girl\n",
    "'''\n",
    "\n",
    "'''\n",
    "Expected value:\n",
    "We will sometimes talk about the expected value of a random variable, which is the average of its values weighted by their probabilities. P1*outcome+P2*outcome…Pn*outcomen\n",
    "The coin flip variable has an expected value of 1/2 (= 0 * 1/2 + 1 * 1/2), \n",
    "and the range(10) variable has an expected value of 4.5. 4.5=(.1*0+.1*1…+.1*9)\n",
    "'''\n",
    "\n",
    "def uniform_pdf(x):\n",
    "    return 1 if x >= 0 and x < 1 else 0\n",
    "\n",
    "def uniform_cdf(x): \n",
    "    \"returns the probability that a uniform random variable is <= x\"\n",
    "    if x < 0: return 0 \t        # uniform random is never less than 0\n",
    "    elif x < 1: return x \t# e.g. P(X <= 0.4) = 0.4\n",
    "    else: return 1 \t\t# uniform random is always less than 1\n",
    "\n",
    "\n",
    "# It tells probability at a particular point. Max p should be at x=0 for mu=0 and sigma=1 as 0 will be mean.\n",
    "def normal_pdf(x, mu=0, sigma=1):  \n",
    "    sqrt_two_pi = math.sqrt(2 * math.pi)\n",
    "    return (math.exp(-(x-mu) ** 2 / 2 / sigma ** 2) / (sqrt_two_pi * sigma))\n",
    "\n",
    "# It tells probability from start to a particular point. Probability lies with some standard deviations.\n",
    "'''The cumulative distribution function for the normal distribution cannot be written in an \n",
    "’elementary’ manner, but we can write it using Python’s math.erf: '''\n",
    "def normal_cdf(x, mu=0,sigma=1):\n",
    "    return (1 + math.erf((x - mu) / math.sqrt(2) / sigma)) / 2\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Sometimes we’ll need to invert normal_cdf to find the value corresponding to a specified probability. There’s no simple way to compute its inverse,\n",
    "but normal_cdf is continuous and strictly increasing, so we can use a binary search: '''\n",
    "# The function repeatedly bisects intervals until it narrows in on a Z that’s close enough to the desired probability.\n",
    "# To do ????\n",
    "def inverse_normal_cdf(p, mu=0, sigma=1, tolerance=0.00001):\n",
    "    \"\"\"find approximate inverse using binary search\"\"\"\n",
    "    # if not standard, compute standard and rescale\n",
    "    if mu != 0 or sigma != 1:\n",
    "        return mu + sigma * inverse_normal_cdf(p, tolerance=tolerance)\n",
    "    low_z, low_p = -10.0, 0          # normal_cdf(-10) is (very close to) 0\n",
    "    hi_z, hi_p = 10.0, 1             # normal_cdf(10) is (very close to) 1\n",
    "    while hi_z - low_z > tolerance:\n",
    "        mid_z = (low_z + hi_z) / 2   # consider the midpoint\n",
    "        mid_p = normal_cdf(mid_z)    # and the cdf's value there\n",
    "        if mid_p < p:\n",
    "            # midpoint is still too low, search above it\n",
    "            low_z, low_p = mid_z, mid_p\n",
    "        elif mid_p > p:\n",
    "            # midpoint is still too high, search below it\n",
    "            hi_z, hi_p = mid_z, mid_p\n",
    "        else:\n",
    "            break\n",
    "    return mid_z\n",
    "\n",
    "\n",
    "'''An easy way to illustrate this is by looking at binomial random variables, which have two parameters n and p. A Binomial(n,p) random variable is\n",
    "simply the sum of n independent Bernoulli(p) random variables, each of which equals 1 with probability p and 0 with probability 1-p: '''\n",
    "# \n",
    "def bernoulli_trial(p):\n",
    "    return 1 if random.random() < p else 0\n",
    "\n",
    "def binomial(n, p):\n",
    "    return sum(bernoulli_trial(p) for _ in range(n))\n",
    "\n",
    "# 75 should have most value.\n",
    "# data=Counter([binomial(100, .75) for _ in range(100)])\n",
    "# print data\n",
    "\n",
    "\n",
    "# ???? Later\n",
    "def make_hist(p, n, num_points):\n",
    "    data = [binomial(n, p) for _ in range(num_points)]\n",
    "    # use a bar chart to show the actual binomial samples\n",
    "    histogram = Counter(data)\n",
    "    plt.bar([x - 0.4 for x in histogram.keys()],\n",
    "            [v / num_points for v in histogram.values()],\n",
    "            0.8,\n",
    "            color='0.75')\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(n * p * (1 - p))\n",
    "    # use a line chart to show the normal approximation\n",
    "    xs = range(min(data), max(data) + 1)\n",
    "    ys = [normal_cdf(i + 0.5, mu, sigma) - normal_cdf(i - 0.5, mu, sigma)\n",
    "            for i in xs]\n",
    "    plt.plot(xs,ys)\n",
    "    plt.title(\"Binomial Distribution vs. Normal Approximation\")\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOn the other hand, if \\xe2\\x80\\x9cless bias\\xe2\\x80\\x9d only got 150 clicks, we\\xe2\\x80\\x99d have:\\nz = a_b_test_statistic(1000, 200, 1000, 150) \\t# -2.94\\ntwo_sided_p_value(z)\\t\\t\\t\\t# 0.003\\nwhich means there\\xe2\\x80\\x99s only a 0.003 probability you\\xe2\\x80\\x99d see such a large difference if the ads were equally effective.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#***************************************************************************************************\n",
    "#**********************Chapter 7. Hypothesis and Inference****************************************\n",
    "#***************************************************************************************************\n",
    "'''\n",
    "Example: Flipping a Coin\n",
    "Imagine we have a coin and we want to test whether it’s fair. We’ll make the assumption that the coin has some probability p of landing heads,\n",
    "and so our null hypothesis is that the coin is fair — that is, that p=0.5. We’ll test this against the alternative hypothesis p != .5.\n",
    "In particular, our test will involve flipping the coin some number n times and counting the number of heads X. Each coin flip is a Bernoulli trial,\n",
    "which means that X is a Binomial(n,p) random variable, which (as we saw in Chapter 6) we can approximate using the normal distribution:\n",
    "'''\n",
    "# \n",
    "def normal_approximation_to_binomial(n, p):\n",
    "    \"\"\"finds mu and sigma corresponding to a Binomial(n, p)\"\"\"\n",
    "    mu = p * n\n",
    "    sigma = math.sqrt(p * (1 - p) * n)\n",
    "    return mu, sigma\n",
    "\n",
    "# the normal cdf _is_ the probability the variable is below a threshold\n",
    "normal_probability_below = normal_cdf\n",
    "\n",
    "# it's above the threshold if it's not below the threshold\n",
    "def normal_probability_above(lo, mu=0, sigma=1):\n",
    "    return 1 - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# it's between if it's less than hi, but not less than lo\n",
    "def normal_probability_between(lo, hi, mu=0, sigma=1):\n",
    "    return normal_cdf(hi, mu, sigma) - normal_cdf(lo, mu, sigma)\n",
    "\n",
    "# it's outside if it's not between\n",
    "def normal_probability_outside(lo, hi, mu=0, sigma=1):\n",
    "    return 1 - normal_probability_between(lo, hi, mu, sigma)\n",
    "\n",
    "'''\n",
    "We can also do the reverse — find either the nontail region or the (symmetric) interval around the mean that accounts for a certain level of likelihood.\n",
    "For example, if we want to find an interval centered at the mean and containing 60% probability, then we find the cutoffs where the upper and\n",
    "lower tails each contain 20% of the probability (leaving 60%): '''\n",
    "# ???? First understand inverse_normal_cdf in previos chapter.\n",
    "def normal_upper_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for which P(Z <= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(probability, mu, sigma)\n",
    "\n",
    "def normal_lower_bound(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the z for which P(Z >= z) = probability\"\"\"\n",
    "    return inverse_normal_cdf(1 - probability, mu, sigma)\n",
    "\n",
    "def normal_two_sided_bounds(probability, mu=0, sigma=1):\n",
    "    \"\"\"returns the symmetric (about the mean) bounds that contain the specified probability\"\"\"\n",
    "    tail_probability = (1 - probability) / 2\n",
    "    # upper bound should have tail_probability above it\n",
    "    upper_bound = normal_lower_bound(tail_probability, mu, sigma)\n",
    "    # lower bound should have tail_probability below it\n",
    "    lower_bound = normal_upper_bound(tail_probability, mu, sigma)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "'''\n",
    "In particular, let’s say that we choose to flip the coin n=1000 times. If our hypothesis of fairness is true, X should be distributed approximately\n",
    "normally with mean 500 and standard deviation 15.8: '''\n",
    "mu_0, sigma_0 = normal_approximation_to_binomial(1000, 0.5)\n",
    "# print \"mu_0 which is n*p and sigma_0(SD) which is root[n*p*(1-p)] for 1000 tosses and prob 0.5 are %r and %r\" %(mu_0,sigma_0)\n",
    "# print normal_two_sided_bounds(0.95, mu_0, sigma_0)    O/P is (469.01026640487555, 530.9897335951244)\n",
    "# print normal_two_sided_bounds(0.95, 0, 1)             O/P is (-1.9599628448486328, 1.9599628448486328)\n",
    "\n",
    "\n",
    "def two_sided_p_value(x, mu=0, sigma=1):\n",
    "    if x >= mu:\n",
    "        # if x is greater than the mean, the tail is what's greater than x\n",
    "#        print normal_probability_above(x, mu, sigma)\n",
    "        return 2 * normal_probability_above(x, mu, sigma)\n",
    "    else:\n",
    "        # if x is less than the mean, the tail is what's less than x\n",
    "        return 2 * normal_probability_below(x, mu, sigma)\n",
    "\n",
    "# If we were to see 530 heads, we would compute:\n",
    "# print two_sided_p_value(529.5, mu_0, sigma_0)              # 0.062\n",
    "\n",
    "\n",
    "# Awesome Simulation: One way to convince yourself that this is a sensible estimate is with a simulation:\n",
    "# We flip coin 1000 time for 100000 trials and see value of the extreme_value_count / 100000 which gives probability of values out of interval 470 and 530.\n",
    "'''\n",
    "extreme_value_count = 0\n",
    "for _ in range(100000):\n",
    "    num_heads = sum(1 if random.random() < 0.5 else 0 \t\t# count # of heads\n",
    "                    for _ in range(1000)) \t\t        # in 1000 flips\n",
    "    if num_heads >= 530 or num_heads <= 470: \t\t\t# and count how often\n",
    "        extreme_value_count += 1 \t\t\t        # the # is 'extreme'\n",
    " print extreme_value_count / 100000 # 0.062\n",
    "'''\n",
    "\n",
    "\n",
    "# Confidence Intervals\n",
    "# main confusion comes why 1000 in this math.sqrt(p * (1 - p) / 1000).\n",
    "# 1000 comes because it shuld be sigma/root(n) for sampling distribution.\n",
    "# Here we don’t know p, so instead we use our estimate:\n",
    "'''\n",
    "p_hat = 525 / 1000\n",
    "mu = p_hat\n",
    "sigma = math.sqrt(p_hat * (1 - p_hat) / 1000)\t  # 0.0158\n",
    "print sigma\n",
    "print normal_two_sided_bounds(0.95, mu, sigma) \n",
    "'''\n",
    "\n",
    "\n",
    "# P-hacking: A procedure that erroneously rejects the null hypothesis only 5% of the time will —\n",
    "# By definition — 5% of the time erroneously reject the null hypothesis:\n",
    "def run_experiment():\n",
    "    \"\"\"flip a fair coin 1000 times, True = heads, False = tails\"\"\"\n",
    "    return [random.random() < .5 for _ in range(1000)]\n",
    "\n",
    "def reject_fairness(experiment):\n",
    "    \"\"\"using the 5% significance levels\"\"\"\n",
    "    num_heads = len([flip for flip in experiment if flip])\n",
    "    return num_heads < 469 or num_heads > 531\n",
    "\n",
    "# random.seed(0)\n",
    "# experiments = [run_experiment() for _ in range(1000)]\n",
    "# num_rejections = len([experiment for experiment in experiments if reject_fairness(experiment)])\n",
    "# print num_rejections # 46\n",
    "\n",
    "#other way:\n",
    "def run_experiment1():\n",
    "    \"\"\"flip a fair coin 1000 times, True = heads, False = tails\"\"\"\n",
    "    sum1 = sum([random.random() < .5 for _ in range(1000)])\n",
    "    if (sum1 > 469 and sum1 < 531):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "#random.seed(0)\n",
    "#experiments1 = sum(1 - run_experiment1() for _ in range(1000))\n",
    "#print experiments1\n",
    "\n",
    "\n",
    "# Example: Running an A/B Test: It is like measuring if 2 samples belong to same population.\n",
    "def estimated_parameters(N, n):\n",
    "    p = n / N\n",
    "    sigma = math.sqrt(p * (1 - p) / N)\n",
    "    return p, sigma\n",
    "\n",
    "def a_b_test_statistic(N_A, n_A, N_B, n_B):\n",
    "    p_A, sigma_A = estimated_parameters(N_A, n_A)\n",
    "    p_B, sigma_B = estimated_parameters(N_B, n_B)\n",
    "    return (p_B - p_A) / math.sqrt(sigma_A ** 2 + sigma_B ** 2)\n",
    "\n",
    "# For example, if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180 clicks out of 1,000 views, the statistic equals:\n",
    "# z = a_b_test_statistic(1000, 200, 1000, 180)\t # -1.14\n",
    "# The probability of seeing such a large difference if the means were actually equal would be:\n",
    "'''\n",
    "For example, if “tastes great” gets 200 clicks out of 1,000 views and “less bias” gets 180 clicks out of 1,000 views, the statistic equals:\n",
    "z = a_b_test_statistic(1000, 200, 1000, 180)\t # -1.14\n",
    "The probability of seeing such a large difference if the means were actually equal would be:\n",
    "two_sided_p_value(z) \t\t\t# 0.254\n",
    "which is large enough that you can’t conclude there’s much of a difference. \n",
    "'''\n",
    "\n",
    "'''\n",
    "On the other hand, if “less bias” only got 150 clicks, we’d have:\n",
    "z = a_b_test_statistic(1000, 200, 1000, 150) \t# -2.94\n",
    "two_sided_p_value(z)\t\t\t\t# 0.003\n",
    "which means there’s only a 0.003 probability you’d see such a large difference if the ads were equally effective.\n",
    "'''\n",
    "\n",
    "# Bayesian Inference:\n",
    "# To do ????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************\n",
    "# **********************Chapter 8. Gradient Descent*******************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "\"\"\"NOTE\n",
    "If a function has a unique global minimum, this procedure is likely to find it.\n",
    "If a function has multiple (local) minima, this procedure might ’find’ the wrong one of them,\n",
    "in which case you might re-run the procedure from a variety of starting points. \n",
    "If a function has no minimum, then it’s possible the procedure might go on forever.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Estimating the Gradient:\n",
    "If f is a function of one variable, its derivative at a point x measures how f(x) changes when we make a very small change to x.\n",
    "It is defined as the limit of the difference quotients:\n",
    "\"\"\"\n",
    "def difference_quotient(f,x,h):\n",
    "    return (f(x+h)-f(x))/h     # as h approaches zero.\n",
    "\n",
    "def square(x):\n",
    "    return x*x\n",
    "\n",
    "#has the derivative. This is other way than doing difference_quotient for a very small h.\n",
    "def derivative(x):\n",
    "    return 2*x\n",
    "\n",
    "derivative_estimate=partial(difference_quotient,square,h=.000001)\n",
    "'''\n",
    "x=range(-10,10)\n",
    "y1=map(derivative,x)\n",
    "y2=map(derivative_estimate,x)\n",
    "print 'y1 is ', y1\n",
    "print 'y2 is ', y2\n",
    "'''\n",
    "\n",
    "'''\n",
    "When f is a function of many variables, it has multiple partial derivatives, each indicating how f changes when we make small changes\n",
    "in just one of the input variables.\n",
    "We calculate its ith partial derivative by treating it as a function of just its ith variable, holding the other variables fixed:\n",
    "'''\n",
    "\n",
    "def partial_difference_quotient(f, v, i, h):\n",
    "    \"\"\"compute the ith partial difference quotient of f at v\"\"\"\n",
    "    w = [v_j + (h if j == i else 0) # add h to just the ith element of v\n",
    "            for j, v_j in enumerate(v)]\n",
    "    return (f(w) - f(v)) / h\n",
    "\n",
    "# after which we can estimate the gradient the same way:\n",
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return [partial_difference_quotient(f, v, i, h)\n",
    "                for i, _ in enumerate(v)]\n",
    "\n",
    "\"\"\"\n",
    "NOTE:\n",
    "A major drawback to this “estimate using difference quotients” approach is that it’s computationally expensive.\n",
    "If v has length n, estimate_gradient has to evaluate f on 2n different inputs.\n",
    "If you’re repeatedly estimating gradients, you’re doing a whole lot of extra work.\n",
    "\"\"\"\n",
    "\n",
    "'''\n",
    "Using the Gradient:\n",
    "It’s easy to see that the sum_of_squares function is smallest when its input v is a vector of zeroes. But imagine we didn’t know that.\n",
    "Let’s use gradients to find the minimum among all three-dimensional vectors. We’ll just pick a random starting point and then take tiny\n",
    "steps in the opposite direction of the gradient until we reach a point where the gradient is very small:\n",
    "'''\n",
    "\n",
    "def step(v, gradient, step_size):\n",
    "    \"\"\"move step_size in the direction from v\"\"\"\n",
    "    return[v_i+step_size*gradient_i for v_i,gradient_i in zip(v,gradient)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    \"\"\"derivative of sum of squares function at a point\"\"\"\n",
    "    return [2*v_i for v_i in v]\n",
    "'''\n",
    "# pick a random starting point\n",
    "v = [random.randint(-10,10) for i in range(3)]\n",
    "tolerance = 0.0000001\n",
    "print 'start v is ', v\n",
    "'''\n",
    "'''\n",
    "while True:\n",
    "    global v\n",
    "    gradient = sum_of_squares_gradient(v)\n",
    "    next_v = step(v,gradient,-0.001)           \n",
    "    if distance(next_v,v) < tolerance:\n",
    "        break\n",
    "    v=next_v\n",
    "# print 'minima is at', v\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "It is possible that certain step sizes will result in invalid inputs for our function. So we’ll need to create a “safe apply” function that returns\n",
    "infinity (which should never be the minimum of anything) for invalid inputs: '''\n",
    "\n",
    "# ????\n",
    "def safe(f):\n",
    "    \"\"\"return a new function that's the same as f, except that it outputs infinity whenever f produces an error\"\"\"\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf') # this means \"infinity\" in Python\n",
    "    return safe_f\n",
    "\n",
    "\"\"\"\n",
    "Putting It All Together:\n",
    "In the general case, we have some target_fn that we want to minimize, and we also have its gradient_fn.\n",
    "For example, the target_fn could represent the errors in a model as a function of its parameters, and we might want to find the parameters\n",
    "that make the errors as small as possible.\n",
    "Furthermore, let’s say we have (somehow) chosen a starting value for the parameters theta_0. Then we can implement gradient descent as:\n",
    "\"\"\"\n",
    "# ???? Below can be tested on a function.\n",
    "def minimize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    \"\"\"use gradient descent to find theta that minimizes target function\"\"\"\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    theta = theta_0             # set theta to initial value\n",
    "    target_fn = safe(target_fn) # safe version of target_fn\n",
    "    value = target_fn(theta)    # value we're minimizing\n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                        for step_size in step_sizes]\n",
    "        # choose the one that minimizes the error function\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        # stop if we're \"converging\"\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value\n",
    "\n",
    "\"\"\"\n",
    "We called it minimize_batch because, for each gradient step, it looks at the entire data set (because target_fn returns the error on the whole data set).\n",
    "In the next section, we’ll see an alternative approach that only looks at one data point at a time.\n",
    "Sometimes we’ll instead want to maximize a function, which we can do by minimizing its negative (which has a corresponding negative gradient):\n",
    "\"\"\"\n",
    "# ???? Below ones\n",
    "def negate(f):\n",
    "    \"\"\"return a function that for any input x returns -f(x)\"\"\"\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    \"\"\"the same when f returns a list of numbers\"\"\"\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minimize_batch(negate(target_fn),\n",
    "                            negate_all(gradient_fn),\n",
    "                            theta_0,\n",
    "                            tolerance)\n",
    "\n",
    "\n",
    "# Stochastic Gradient Descent:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#***************************************************************************************************\n",
    "# **********************Chapter 9. Getting Data*****************************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "# ???? Skipped for time being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# _, num_columns = shape(data)\\n# fig, ax = plt.subplots(num_columns, num_columns)\\n\\nfor i in range(num_columns):\\n    for j in range(num_columns):\\n        # scatter column_j on the x-axis vs column_i on the y-axis\\n        if i != j: ax[i][j].scatter(get_column(data, j), get_column(data, i))\\n        # unless i == j, in which case show the series name\\n        else: ax[i][j].annotate(\"series \" + str(i), (0.5, 0.5),\\n        xycoords=\\'axes fraction\\',\\n        ha=\"center\", va=\"center\")\\n        # then hide axis labels except left and bottom charts\\n        if i < num_columns - 1: ax[i][j].xaxis.set_visible(False)\\n        if j > 0: ax[i][j].yaxis.set_visible(False)\\n# fix the bottom right and top left axis labels, which are wrong because their charts only have text in them\\nax[-1][-1].set_xlim(ax[0][-1].get_xlim())\\nax[0][0].set_ylim(ax[0][1].get_ylim())\\nplt.show()\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#***************************************************************************************************\n",
    "# **********************Chapter 10. Working with Data***********************************************\n",
    "#***************************************************************************************************\n",
    "\n",
    "#???? Skipping visualization part for time being. Have to complete once install matplotlib or move code to Jupyter Notebook.\n",
    "# But printing values instead of plots.\n",
    "\n",
    "\n",
    "# Exploring One-Dimensional Data\n",
    "# An obvious first step is to compute a few summary statistics. You’d like to know how many data points you have, the smallest, the largest,\n",
    "# the mean, and the standard deviation. But even these don’t necessarily give you a great understanding. A good next step is to create a histogram,\n",
    "# in which you group your data into discrete buckets and count how many points fall into each bucket:\n",
    "def bucketize(point, bucket_size):\n",
    "    \"\"\"floor the point to the next lower multiple of bucket_size\"\"\"\n",
    "    return bucket_size * math.floor(point / bucket_size)\n",
    "\n",
    "def make_histogram(points, bucket_size):\n",
    "    \"\"\"buckets the points and counts how many in each bucket\"\"\"\n",
    "    return Counter(bucketize(point, bucket_size) for point in points)\n",
    "\n",
    "def plot_histogram(points, bucket_size, title=\"\"):\n",
    "    histogram = make_histogram(points, bucket_size)\n",
    "    plt.bar(histogram.keys(), histogram.values(), width=bucket_size)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Defining as can not plot without matplotlib.\n",
    "def plot_histogram(points, bucket_size=10):\n",
    "    histogram = make_histogram(points, bucket_size)\n",
    "    print histogram\n",
    "\n",
    "# uniform between -100 and 100\n",
    "# uniform = [100 * random.random() - 100 for _ in range(10000)]\n",
    "# print uniform\n",
    "# plot_histogram(uniform)\n",
    "\n",
    "# normal distribution with mean 0, standard deviation 50\n",
    "# ???? First understand inverse_normal_cdf\n",
    "# normal = [50 * inverse_normal_cdf(random.random()) for _ in range(10000)]\n",
    "# plot_histogram(normal, 10)\n",
    "\n",
    "\n",
    "# Exploring 2-Dimensional Data\n",
    "'''\n",
    "# For example, consider another fake data set:\n",
    "def random_normal():\n",
    "    \"\"\"returns a random draw from a standard normal distribution\"\"\"\n",
    "    return inverse_normal_cdf(random.random())\n",
    "xs = [random_normal() for _ in range(1000)]\n",
    "ys1 = [ x + random_normal() / 2 for x in xs]\n",
    "ys2 = [-x + random_normal() / 2 for x in xs]\n",
    "# If you were to run plot_histogram on ys1 and ys2 you’d get very similar looking plots\n",
    "# (indeed, both are normally distributed with the same mean and standard deviation).\n",
    "\n",
    "# Figure 10-2. Histogram of normal\n",
    "# But each has a very different joint distribution with xs, as shown in Figure 10-3:\n",
    "plt.scatter(xs, ys1, marker='.', color='black', label='ys1')\n",
    "plt.scatter(xs, ys2, marker='.', color='gray', label='ys2')\n",
    "plt.xlabel('xs')\n",
    "plt.ylabel('ys')\n",
    "plt.legend(loc=9)\n",
    "plt.title(\"Very Different Joint Distributions\")\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "# Many Dimensions\n",
    "# With many dimensions, you’d like to know how all the dimensions relate to one another. A simple approach is to look at the correlation matrix,\n",
    "# in which the entry in row i and column j is the correlation between the ith dimension and the jth dimension of the data:\n",
    "\n",
    "def correlation_matrix(data):\n",
    "    \"\"\"returns the num_columns x num_columns matrix whose (i, j)th entry\n",
    "    is the correlation between columns i and j of data\"\"\"\n",
    "    _, num_columns = shape(data)\n",
    "    def matrix_entry(i, j):\n",
    "        return correlation(get_column(data, i), get_column(data, j))\n",
    "    return make_matrix(num_columns, num_columns, matrix_entry)\n",
    "\n",
    "'''\n",
    "random.seed(2)\n",
    "a = [[int(10*random.random()) for i in range(7)] for j in range(6)]\n",
    "# print a\n",
    "# print correlation_matrix([[int(10*random.random()) for i in range(7)] for j in range(6)])\n",
    "'''\n",
    "\n",
    "# A more visual approach (if you don’t have too many dimensions) is to make a scatterplot matrix (Figure 10-4) showing all the pairwise scatterplots.\n",
    "# To do that we’ll use plt.subplots(), which allows us to create subplots of our chart. We give it the number of rows and the number of columns,\n",
    "# and it returns a figure object (which we won’t use) and a two-dimensional array of axes objects (each of which we’ll plot to):\n",
    "# Once matplotlib available ???? \n",
    "\n",
    "\"\"\"\n",
    "# _, num_columns = shape(data)\n",
    "# fig, ax = plt.subplots(num_columns, num_columns)\n",
    "\n",
    "for i in range(num_columns):\n",
    "    for j in range(num_columns):\n",
    "        # scatter column_j on the x-axis vs column_i on the y-axis\n",
    "        if i != j: ax[i][j].scatter(get_column(data, j), get_column(data, i))\n",
    "        # unless i == j, in which case show the series name\n",
    "        else: ax[i][j].annotate(\"series \" + str(i), (0.5, 0.5),\n",
    "        xycoords='axes fraction',\n",
    "        ha=\"center\", va=\"center\")\n",
    "        # then hide axis labels except left and bottom charts\n",
    "        if i < num_columns - 1: ax[i][j].xaxis.set_visible(False)\n",
    "        if j > 0: ax[i][j].yaxis.set_visible(False)\n",
    "# fix the bottom right and top left axis labels, which are wrong because their charts only have text in them\n",
    "ax[-1][-1].set_xlim(ax[0][-1].get_xlim())\n",
    "ax[0][0].set_ylim(ax[0][1].get_ylim())\n",
    "plt.show()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Cleaning and Munging: skipped because ???? pip install dateutil is failing with belong error.\n",
    "# Could not find a version that satisfies the requirement dateutil (from versions: ) No matching distribution found for dateutil\n",
    "\n",
    "\n",
    "# Manipulating Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
