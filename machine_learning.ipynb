{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import Counter\n",
    "import math, random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Overfitting and Underfitting:\n",
    "# The simplest way to do this is to split your data set, so that (for example) two-thirds of it is used to train\n",
    "# the model, after which we measure the model's performance on the remaining third:\n",
    "def split_data(data, prob):\n",
    "    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    results = [], []   # Tuple of 2 lists\n",
    "#    print results\n",
    "    for row in data:\n",
    "        results[0 if random.random() < prob else 1].append(row)\n",
    "    return results\n",
    "'''\n",
    "X=[[100*random.random() for i in range(4)] for j in range(10)] # Create 4 * 10 matrix.\n",
    "for i in X:\n",
    "    print i\n",
    "y=[[100*random.random() for i in range(1)] for j in range(10)] \n",
    "for i in y:\n",
    "    print i\n",
    "a=zip(X,y)\n",
    "print a\n",
    "'''\n",
    "\n",
    "def train_test_split(x, y, test_pct):\n",
    "    data = zip(x, y)\t\t\t\t               # pair corresponding values\n",
    "    train, test = split_data(data, 1 - test_pct)   # split the data set of pairs\n",
    "    print train\n",
    "    print test\n",
    "    x_train, y_train = zip(*train)\t\t           # magical un-zip trick ???? later\n",
    "    x_test, y_test = zip(*test)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "'''\n",
    "X=[[100*random.random() for i in range(4)] for j in range(10)] # Create 4 * 10 matrix.\n",
    "y=[[100*random.random() for i in range(1)] for j in range(10)] \n",
    "a=train_test_split(X,y,.10)\n",
    "'''\n",
    "\n",
    "\n",
    "# Correctness:\n",
    "\n",
    "# As we'll see below, this test is indeed more than 98% accurate. Nonetheless, it's an incredibly stupid test,\n",
    "# and a good illustration of why we don't typically use \"accuracy\" to measure how good a model is.\n",
    "# We often represent these as counts in a confusion matrix:\n",
    "#      \t\t\t\t        Spam \t\t\t    not Spam\n",
    "# predict \"Spam\" \t\t    True Positive \t\tFalse Positive\n",
    "# predict \"Not Spam\"\t\tFalse Negative \t\tTrue Negative\n",
    "\n",
    "#              leukemia \tno leukemia \ttotal\n",
    "# \"Luke\" \t\t70 \t\t    4,930 \t\t    5,000\n",
    "# not \"Luke\" \t13,930 \t\t981,070 \t    995,000\n",
    "# total \t\t14,000 \t\t986,000\t        1,000,000\n",
    "\n",
    "# We can then use these to compute various statistics about model performance. \n",
    "# For example ,accuracy is defined as the fraction of correct predictions:\n",
    "def accuracy(tp, fp, fn, tn):\n",
    "    correct = tp + tn\n",
    "    total = tp + fp + fn + tn\n",
    "    return correct / total\n",
    "# print accuracy(70, 4930, 13930, 981070) \t# 0.98114 That's why only accuracy is stupid.\n",
    "\n",
    "# That seems like a pretty impressive number. But clearly this is not a good test, which means that we probably \n",
    "# shouldn't put a lot of credence in raw accuracy.\n",
    "# It's common to look at the combination of precision and recall. Precision measures how accurate our positive predictions were:\n",
    "def precision(tp, fp, fn, tn):\n",
    "    return tp / (tp + fp)\n",
    "# print precision(70, 4930, 13930, 981070)\t # 0.014\n",
    "\n",
    "# And recall measures what fraction of the positives our model identified:\n",
    "def recall(tp, fp, fn, tn):\n",
    "    return tp / (tp + fn)\n",
    "# print recall(70, 4930, 13930, 981070) \t               # 0.005\n",
    "\n",
    "# These are both terrible numbers, reflecting that this is a terrible model.\n",
    "\n",
    "# Sometimes precision and recall are combined into the F1 score, which is defined as:\n",
    "def f1_score(tp, fp, fn, tn):\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2 * p * r / (p + r)\n",
    "# This is the harmonic mean of precision and recall and necessarily lies between them.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
